{% set name = "cw-eval" %}
{% set version = "0.3" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name|replace("-","_") }}-{{ version }}.tar.gz
  sha256: b4119b087a75013446337c9ab2873b43c6a654730b0962c6a8504235caaf4088


build:
  number: 0
  script: "{{ PYTHON }} -m pip install . --no-deps -vv"

requirements:
  build:
    - {{ compiler('c') }}
  host:
    - python
    - pip
  run:
    - python
    - rtree
    - shapely
    - pandas
    - geopandas
    - tqdm

test:
  imports:
    cw-eval
    cw-eval.challenge_eval

about:
  home: http://github.com/cosmiq/cw-eval
  license: Apache-2.0
  license_family: Apache
  license_file: LICENSE.txt
  summary: 'Evaluation metrics for Geospatial Machine Learning Challenges'

  description: |
    cw-eval is an evaluation suite for scoring entries in geospatial image
    analysis competitions. It includes tools for calculating IoU scores,
    precision, recall, F1 score, and scripts to score entire entries in either
    geojson or csv formats.
  doc_url: http://cw-eval.readthedocs.io/
  dev_url: https://github.com/cosmiq/cw-eval

extra:
  recipe-maintainers:
    - nrweir
