{% set version = '0.4.1' %}
{% set posix = 'm2-' if win else '' %}
{% set native = 'm2w64-' if win else '' %}

package:
  name: r-lime
  version: {{ version|replace("-", "_") }}

source:
  url:
    - {{ cran_mirror }}/src/contrib/lime_{{ version }}.tar.gz
    - {{ cran_mirror }}/src/contrib/Archive/lime/lime_{{ version }}.tar.gz
  sha256: 34fa0c87a6b79f60aed156ba50707e6abb12cd7cc11d7796d767a02c6fc69ee3

build:
  merge_build_host: True  # [win]
  number: 0
  rpaths:
    - lib/R/lib/
    - lib/

requirements:
  build:
    - {{ compiler('c') }}        # [not win]
    - {{ compiler('cxx') }}      # [not win]
    - {{native}}toolchain        # [win]
    - {{posix}}filesystem        # [win]
    - {{posix}}make
    - {{posix}}sed               # [win]
    - {{posix}}coreutils         # [win]
    - {{posix}}zip               # [win]
  host:
    - r-base
    - r-matrix
    - r-rcpp
    - r-rcppeigen
    - r-assertthat
    - r-ggplot2
    - r-glmnet
    - r-gower
    - r-htmlwidgets
    - r-shiny
    - r-shinythemes
    - r-stringi
  run:
    - r-base
    - {{native}}gcc-libs         # [win]
    - r-matrix
    - r-rcpp
    - r-rcppeigen
    - r-assertthat
    - r-ggplot2
    - r-glmnet
    - r-gower
    - r-htmlwidgets
    - r-shiny
    - r-shinythemes
    - r-stringi

test:
  commands:
    - $R -e "library('lime')"           # [not win]
    - "\"%R%\" -e \"library('lime')\""  # [win]

about:
  home: https://github.com/thomasp85/lime
  license: MIT
  summary: When building complex models, it is often difficult to explain why the model should
    be trusted. While global measures such as accuracy are useful, they cannot be used
    for explaining why a model made a specific prediction. 'lime' (a port of the 'lime'
    'Python' package) is a method for explaining the outcome of black box models by
    fitting a local model around the point in question an perturbations of this point.
    The approach is described in more detail in the article by Ribeiro et al. (2016)  <arXiv:1602.04938>.
  license_family: MIT

extra:
  recipe-maintainers:
    - conda-forge/r

# Package: lime
# Type: Package
# Title: Local Interpretable Model-Agnostic Explanations
# Version: 0.4.1
# Date: 2018-11-21
# Authors@R: c(person('Thomas Lin', 'Pedersen', role = c('cre', 'aut'), email = "thomasp85@gmail.com"), person('Michael', 'Benesty', role = c('aut'), email = "michael@benesty.fr"))
# Maintainer: Thomas Lin Pedersen <thomasp85@gmail.com>
# Description: When building complex models, it is often difficult to explain why the model should be trusted. While global measures such as accuracy are useful, they cannot be used for explaining why a model made a specific prediction. 'lime' (a port of the 'lime' 'Python' package) is a method for explaining the outcome of black box models by fitting a local model around the point in question an perturbations of this point. The approach is described in more detail in the article by Ribeiro et al. (2016)  <arXiv:1602.04938>.
# License: MIT + file LICENSE
# URL: https://github.com/thomasp85/lime
# BugReports: https://github.com/thomasp85/lime/issues
# Encoding: UTF-8
# LazyData: true
# RoxygenNote: 6.1.1
# VignetteBuilder: knitr
# Imports: glmnet, stats, ggplot2, tools, stringi, Matrix, Rcpp, assertthat, htmlwidgets, shiny, shinythemes, methods, grDevices, gower
# Suggests: xgboost, testthat, mlr, h2o, text2vec, MASS, covr, knitr, rmarkdown, devtools, magick, keras, ranger
# LinkingTo: Rcpp, RcppEigen
# NeedsCompilation: yes
# Packaged: 2018-11-21 12:20:40 UTC; thomas
# Author: Thomas Lin Pedersen [cre, aut], Michael Benesty [aut]
# Repository: CRAN
# Date/Publication: 2018-11-21 12:50:02 UTC
