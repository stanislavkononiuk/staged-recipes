# Note: there are many handy hints in comments in this example -- remove them when you've finalized your recipe

# Jinja variables help maintain the recipe as you'll update the version only here.
# Using the name variable with the URL in line 14 is convenient
# when copying and pasting from another recipe, but not really needed.
{% set name = "sparkflow" %}
{% set version = "0.7.0" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name }}-{{ version }}.tar.gz
  sha256: 532e5055f6d21333460b5162dfddbe8d53408eaca95d89d0654d6a95e6b2cb57

build:
  # Uncomment the following line if the package is pure Python and the recipe is exactly the same for all platforms.
  # It is okay if the dependencies are not built for all platforms/versions, although selectors are still not allowed.
  # See https://conda-forge.org/docs/maintainer/knowledge_base.html#noarch-python for more details.
  # noarch: python
  number: 0
  # If the installation is complex, or different between Unix and Windows, use separate bld.bat and build.sh files instead of this key.
  # By default, the package will be built for the Python versions supported by conda-forge and for all major OSs.
  # Add the line "skip: True  # [py<35]" (for example) to limit to Python 3.5 and newer, or "skip: True  # [not win]" to limit to Windows.
  script: |
    {{ PYTHON }} -m pip install . -vv
    skip: True  # [py<35]
    

requirements:
  host:
    - python =3.6
    - pip
  run:
    - six =1.11.0
    - pandas =0.23.4
    - h5py =2.8.0
    - pillow =4.1.1
    - dill
    - tensorflow =1.10.0
    - flask
    - requests
    - python =3.6

test:
  # Some packages might need a `test/commands` key to check CLI.
  # List all the packages/modules that `run_test.py` imports.
  imports:
    - sparkflow
    - sparkflow.tests

about:
  home: https://github.com/lifeomic/sparkflow
  license: MIT
  license_family: MIT
  license_file: LICENSE
  summary: 'Deep learning on Spark with Tensorflow'

  description: |
    sparkflow is an implementation of TensorFlow on Spark. The goal of this library is to provide a simple,
    understandable interface in using TensorFlow on Spark. With SparkFlow, you can easily integrate your 
    deep learning model with a ML Spark Pipeline. Underneath, SparkFlow uses a parameter server to train the
    TensorFlow network in a distributed manner. Through the api, the user can specify the style of training,
    whether that is Hogwild or async with locking.
  doc_url: https://github.com/lifeomic/sparkflow#documentation
  dev_url: https://github.com/lifeomic/sparkflow

extra:
  recipe-maintainers:
    - IlanaRadinsky
