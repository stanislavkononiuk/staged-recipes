diff --git a/lib/pegasus/python/Pegasus/plots_stats/plots/populate.py b/lib/pegasus/python/Pegasus/plots_stats/plots/populate.py
index bb5194a8b..46226d93f 100755
--- a/lib/pegasus/python/Pegasus/plots_stats/plots/populate.py
+++ b/lib/pegasus/python/Pegasus/plots_stats/plots/populate.py
@@ -151,24 +151,24 @@ def get_workflows_uuid():
 	# expand = True
 	try:
 		expanded_workflow_stats = StampedeStatistics(global_db_url)
-	 	expanded_workflow_stats.initialize(global_top_wf_uuid)
-	 	expanded_workflow_stats.set_job_filter('all')
- 	except:
- 		logger.error("Failed to load the database." + global_db_url )
+		expanded_workflow_stats.initialize(global_top_wf_uuid)
+		expanded_workflow_stats.set_job_filter('all')
+	except:
+		logger.error("Failed to load the database." + global_db_url )
 		sys.exit(1)
- 	#expand = False
- 	try:
-	 	root_workflow_stats = StampedeStatistics(global_db_url , False)
-	 	root_workflow_stats.initialize(global_top_wf_uuid)
-	 	root_workflow_stats.set_job_filter('all')
- 	except:
- 		logger.error("Failed to load the database." + global_db_url )
+	#expand = False
+	try:
+		root_workflow_stats = StampedeStatistics(global_db_url , False)
+		root_workflow_stats.initialize(global_top_wf_uuid)
+		root_workflow_stats.set_job_filter('all')
+	except:
+		logger.error("Failed to load the database." + global_db_url )
 		sys.exit(1)
- 	wf_det = root_workflow_stats.get_workflow_details()[0]
- 	# print workflow statistics
- 	global global_wf_id_uuid_map
- 	global_wf_id_uuid_map[wf_det.wf_id] = global_top_wf_uuid
- 	wf_uuid_list = [global_top_wf_uuid]
+	wf_det = root_workflow_stats.get_workflow_details()[0]
+	# print workflow statistics
+	global global_wf_id_uuid_map
+	global_wf_id_uuid_map[wf_det.wf_id] = global_top_wf_uuid
+	wf_uuid_list = [global_top_wf_uuid]
 	desc_wf_uuid_list = expanded_workflow_stats.get_descendant_workflow_ids()
 	for wf_det in desc_wf_uuid_list:
 		global_wf_id_uuid_map[wf_det.wf_id] = wf_det.wf_uuid
@@ -335,13 +335,13 @@ def get_wf_stats(wf_uuid,expand = False):
 	try:
 		workflow_stampede_stats = StampedeStatistics(global_db_url , expand)
 		workflow_stampede_stats.initialize(wf_uuid)
-        except (connection.ConnectionError, DBAdminError) as e:
-                logger.error("------------------------------------------------------")
-                logger.error(e)
-                sys.exit(1)
+	except (connection.ConnectionError, DBAdminError) as e:
+		logger.error("------------------------------------------------------")
+		logger.error(e)
+		sys.exit(1)
 	except:
- 		logger.error("Failed to load the database." + global_db_url )
- 		logger.warning(traceback.format_exc())
+		logger.error("Failed to load the database." + global_db_url )
+		logger.warning(traceback.format_exc())
 		sys.exit(1)
 	return workflow_stampede_stats
 
@@ -393,7 +393,7 @@ def populate_job_invocation_time_details(wf_info, job_stats, invocation_stats ,d
 	@param date_time_filter date time filter
 	"""
 	formatted_stats_list = plot_utils.convert_stats_to_base_time(job_stats , date_time_filter)
- 	jobs_time_list =[]
+	jobs_time_list =[]
 	for stats in formatted_stats_list:
 		content = [stats['date_format'] , stats['count'],stats['runtime']]
 		jobs_time_list.append(content)
@@ -430,7 +430,7 @@ def setup(submit_dir , config_properties):
 		sys.exit(1)
 	
 	# Create the sqllite db url
-        global_db_url = connection.url_by_submitdir(submit_dir, connection.DBType.WORKFLOW, config_properties)
-        global_top_wf_uuid = connection.get_wf_uuid(submit_dir)
+	global_db_url = connection.url_by_submitdir(submit_dir, connection.DBType.WORKFLOW, config_properties)
+	global_top_wf_uuid = connection.get_wf_uuid(submit_dir)
 	if global_db_url is None:
 		sys.exit(1)
