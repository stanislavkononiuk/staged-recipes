{% set name = "pytorch-pretrained-bert" %}
{% set version = "0.4.0" %}

package:
  name: {{ name|lower }}
  version: {{ version }}

source:
  url: https://pypi.io/packages/source/{{ name[0] }}/{{ name }}/{{ name.replace('-','_') }}-{{ version }}.tar.gz
  sha256: a1bbcf9d2ef5f894b583737bd0bcbd7f4cbffddc4722b255aaf8b93fd43892d3

build:
  skip: True  [not linux or py<35]
  number: 0
  script: "{{ PYTHON }} -m pip install . --no-deps -vv"

requirements:
  host:
    - python
    - pip
  run:
    - python
    - pytorch >=0.4.1
    - numpy
    - boto3
    - requests
    - tqdm

test:
  imports:
    - pytorch_pretrained_bert
  requires:
    - pytest

about:
  home: https://pypi.org/project/pytorch-pretrained-bert/
  license: Apache-2.0
  license_family: APACHE
  license_file: '{{ environ["RECIPE_DIR"] }}/LICENSE'
  summary: "A PyTorch implementation of Google AI's BERT model provided with Google's pre-trained models, examples and utilities."
  description: |
    This repository contains an op-for-op PyTorch reimplementation of Google's TensorFlow repository for the BERT model that was released together with the paper BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding by Jacob Devlin, Ming-Wei Chang, Kenton Lee and Kristina Toutanova. This implementation is provided with Google's pre-trained models, examples, notebooks and a command-line interface to load any pre-trained TensorFlow checkpoint for BERT is also provided.
  doc_url: https://github.com/huggingface/pytorch-pretrained-BERT 
  dev_url: https://github.com/huggingface/pytorch-pretrained-BERT

extra:
  recipe-maintainers:
    - CurtLH
